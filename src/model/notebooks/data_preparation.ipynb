{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import calendar\n",
    "import glob"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_weather_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # keeping data only of the relevant cities (Adilabad, Nizamabad, Karimnagar, Khammam and Warangal)\n",
    "    warangal_varaints = [\"Warangal (U)\", \"Warangal (R)\", \"Warangal\", \"Warangal Rural\", \"Warangal Urban\"]\n",
    "    cities = [\"Adilabad\", \"Nizamabad\", \"Karimnagar\", \"Khammam\"]\n",
    "    cols = [\"district\", \"month\", \"rainfall_cumm\", \"rainfall_avg\", \"temp_min\", \"temp_max\", \"humidity_min\", \"humidity_max\", \"wind_speed_min\", \"wind_speed_max\"]\n",
    "    \n",
    "    warangal_df = df.loc[df[\"district\"].isin(warangal_varaints)]\n",
    "    df = df.loc[df[\"district\"].isin(cities)]\n",
    "\n",
    "    # getting month out of odate attribute\n",
    "    df[\"month\"] = pd.DatetimeIndex(df[\"odate\"], dayfirst=True).month\n",
    "    df[\"month\"] = df[\"month\"].apply(lambda x: calendar.month_abbr[x])\n",
    "\n",
    "    # drop unnecessary rows\n",
    "    df.drop(columns=[\"mandal\",\"odate\"], inplace=True)\n",
    "\n",
    "    # grouping the df by district and month and getting mean values for every month\n",
    "    df = df.groupby(by=[\"district\", \"month\"]).agg({\n",
    "        'rainfall': ['sum', 'mean'],'temp_min':'mean', 'temp_max':'mean', 'humidity_min':'mean',\n",
    "          'humidity_max':'mean', 'wind_speed_min':'mean', 'wind_speed_max':'mean'\n",
    "    }).reset_index()\n",
    "    df.columns = df.columns.droplevel(1)\n",
    "    df.columns = cols\n",
    "\n",
    "    # doing the same operations for warangal_df\n",
    "    warangal_df[\"month\"] = pd.DatetimeIndex(warangal_df[\"odate\"], dayfirst=True).month\n",
    "    warangal_df[\"month\"] = warangal_df[\"month\"].apply(lambda x: calendar.month_abbr[x])\n",
    "    warangal_df.drop(columns=[\"mandal\", \"odate\"], inplace=True)\n",
    "\n",
    "    warangal_df = warangal_df.groupby(by=[\"district\", \"month\"]).agg({\n",
    "        'rainfall': ['sum', 'mean'],'temp_min':'mean', 'temp_max':'mean', 'humidity_min':'mean',\n",
    "          'humidity_max':'mean', 'wind_speed_min':'mean', 'wind_speed_max':'mean'\n",
    "    }).reset_index()\n",
    "    warangal_df.columns = warangal_df.columns.droplevel(1)\n",
    "    warangal_df.columns = cols\n",
    "\n",
    "    # getting mean data for warangal\n",
    "    warangal_df = warangal_df.set_index([\"district\"]).groupby(by=\"month\").agg({\n",
    "        'rainfall_cumm': 'sum', 'rainfall_avg':'mean','temp_min':'mean', 'temp_max':'mean', 'humidity_min':'mean',\n",
    "          'humidity_max':'mean', 'wind_speed_min':'mean', 'wind_speed_max':'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    warangal_df[\"district\"] = \"Warangal\"\n",
    "\n",
    "    # appending warangal data back to df\n",
    "    df = df.append(warangal_df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compile_weather_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "#     # keeping data only of the relevant cities (Adilabad, Nizamabad, Karimnagar, Khammam and Warangal)\n",
    "#     warangal_varaints = [\"Warangal (U)\", \"Warangal (R)\", \"Warangal\", \"Warangal Rural\", \"Warangal Urban\"]\n",
    "#     cities = [\"Adilabad\", \"Nizamabad\", \"Karimnagar\", \"Khammam\"]\n",
    "    \n",
    "#     warangal_df = df.loc[df[\"district\"].isin(warangal_varaints)]\n",
    "#     df = df.loc[df[\"district\"].isin(cities)]\n",
    "\n",
    "#     # getting month out of odate attribute\n",
    "#     df[\"month\"] = pd.DatetimeIndex(df[\"odate\"], dayfirst=True).month\n",
    "#     df[\"month\"] = df[\"month\"].apply(lambda x: calendar.month_abbr[x])\n",
    "\n",
    "#     # drop unnecessary rows\n",
    "#     df.drop(columns=[\"mandal\",\"odate\"], inplace=True)\n",
    "\n",
    "#     # grouping the df by district and month and getting mean values for every month\n",
    "#     df = df.groupby(by=[\"district\", \"month\"]).mean().reset_index()\n",
    "\n",
    "#     # doing the same operations for warangal_df\n",
    "#     warangal_df[\"month\"] = pd.DatetimeIndex(warangal_df[\"odate\"], dayfirst=True).month\n",
    "#     warangal_df[\"month\"] = warangal_df[\"month\"].apply(lambda x: calendar.month_abbr[x])\n",
    "#     warangal_df.drop(columns=[\"mandal\", \"odate\"], inplace=True)\n",
    "#     warangal_df = warangal_df.groupby(by=[\"district\", \"month\"]).mean().reset_index()\n",
    "\n",
    "#     # getting mean data for warangal\n",
    "#     warangal_df = warangal_df.set_index([\"district\"]).groupby(by=\"month\").mean().reset_index()\n",
    "#     warangal_df[\"district\"] = \"Warangal\"\n",
    "\n",
    "#     # appending warangal data back to df\n",
    "#     df = df.append(warangal_df)\n",
    "\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_csv_weather(path: str, year: int):\n",
    "    files_list = glob.glob(path + \"/*.csv\")\n",
    "    col_names = [\"district\", \"mandal\", \"odate\", \"rainfall\", \"temp_min\", \"temp_max\", \"humidity_min\", \"humidity_max\", \"wind_speed_min\", \"wind_speed_max\"]\n",
    "    main_df = pd.read_csv(files_list[0], skiprows=1, names=col_names)\n",
    "\n",
    "    if len(files_list) != 1:\n",
    "        for i in range(1, len(files_list)):\n",
    "            data = pd.read_csv(files_list[i], skiprows=1, names=col_names)\n",
    "            main_df = pd.concat([main_df, data])\n",
    "    \n",
    "    main_df.sort_values(by=[\"district\", \"mandal\", \"odate\"], inplace=True)\n",
    "    # main_df = compile_weather_data(main_df)\n",
    "    main_df.to_csv(f'./Weather_Data/daily_weather_data_{year}.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_csv_weather('./Weather_Data/daily_weather_data_2018/', 2018)\n",
    "make_csv_weather('./Weather_Data/telangana-weather-data-2019--All-2023-02-14_1908/', 2019)\n",
    "make_csv_weather('./Weather_Data/telangana-weather-data-2020--All-2023-02-14_1908/', 2020)\n",
    "make_csv_weather('./Weather_Data/telangana-weather-data-2021-All-2023-02-14_1907/', 2021)\n",
    "make_csv_weather('./Weather_Data/telangana-weather-data-2022-All-2023-02-14_1907/', 2022)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vehicle Purchase Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_csv_vehicles(path: str, year):\n",
    "    files_list = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "    main_df = pd.read_csv(files_list[0], encoding='utf-8')\n",
    "    for i in range(1, len(files_list)):\n",
    "        data = pd.read_csv(files_list[i], encoding='utf-8')\n",
    "        main_df = pd.concat([main_df, data])\n",
    "    \n",
    "    main_df.sort_values(by=\"fromdate\", inplace=True)\n",
    "    main_df.to_csv(f'./Vehicle_Data/vehicle_purchase_data_{year}.csv', header=True, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_csv_vehicles(\"./Vehicle_Data/2019/\", 2019)\n",
    "make_csv_vehicles(\"./Vehicle_Data/2020/\", 2020)\n",
    "make_csv_vehicles(\"./Vehicle_Data/2021/\", 2021)\n",
    "make_csv_vehicles(\"./Vehicle_Data/2022/\", 2022)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Industrial Consumption Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_industry_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # dropping unnecessary columns\n",
    "    df.drop(columns=[\"Division\", \"SubDivision\", \"Section\", \"Area\", \"CatCode\", \"CatDesc\", \"TotServices\", \"BilledServices\"], inplace=True)\n",
    "\n",
    "    # keeping only required cities\n",
    "    df = df.loc[df[\"Circle\"].isin([\"ADILABAD\", \"NIZAMABAD\", \"KARIMNAGAR\", \"KHAMMAM\", \"WARANGAL\"])]\n",
    "\n",
    "    # grouping by district/circle and month and finding total energy consumption\n",
    "    df = df.groupby(by=[\"Circle\", \"Month\"]).sum().reset_index()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_csv_industry(path: str, year: int):\n",
    "    files_list = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "    main_df = pd.read_csv(files_list[0])\n",
    "    main_df[\"Month\"] = files_list[0][67:70]\n",
    "    for i in range(1, len(files_list)):\n",
    "        data = pd.read_csv(files_list[i])\n",
    "        data[\"Month\"] = files_list[i][67:70]\n",
    "        main_df = pd.concat([main_df, data])\n",
    "    \n",
    "    main_df = compile_industry_data(main_df)\n",
    "\n",
    "    main_df['Circle'] = main_df['Circle'].apply(lambda x: x.title())\n",
    "    main_df['Month'] = main_df['Month'].apply(lambda x: x.title())\n",
    "\n",
    "    main_df.to_csv(f'./Industry_Consumption/industrial_consumption_data_{year}.csv', header=[\"district\", \"month\", \"units\", \"load\"], index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_csv_industry('./Industry_Consumption/2019/', 2019)\n",
    "make_csv_industry('./Industry_Consumption/2020/', 2020)\n",
    "make_csv_industry('./Industry_Consumption/2021/', 2021)\n",
    "make_csv_industry('./Industry_Consumption/2022/', 2022)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AQI Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_aqi_data(sheet_name: str) -> None:\n",
    "    df = pd.read_excel('./AQI_Data/aqi_data.xlsx', sheet_name=sheet_name, index_col=None)\n",
    "\n",
    "    # taking average of 2 monitoring stations in warangal\n",
    "    warangal_df = df.loc[df[\"Location\"].isin([\"Kuda, warangal\", \"Mee-Seva, Warangal\"])].drop(columns=\"Location\").mean()\n",
    "    warangal_df['Location'] = \"Warangal\"\n",
    "\n",
    "    df = df.loc[df[\"Location\"].isin([\"Nizamabad\", \"Adilabad\", \"Karimnagar\", \"Khammam\"])]\n",
    "    df = df.append(warangal_df, ignore_index=True)\n",
    "\n",
    "    # taking transpose of df\n",
    "    df = df.T\n",
    "\n",
    "    # making first row as the header and dropping it\n",
    "    df.columns = df.iloc[0]\n",
    "    df.drop(df.index[0], inplace=True)\n",
    "\n",
    "    # flattening 2D df to 1D with index as Location and Month\n",
    "    df = df.T.stack().reset_index()\n",
    "\n",
    "    df.to_csv(f'./AQI_Data/monthly_aqi_data_{sheet_name}.csv', header=[\"district\", \"month\", \"aqi\"], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "compile_aqi_data(sheet_name=\"2017\")\n",
    "compile_aqi_data(sheet_name=\"2018\")\n",
    "compile_aqi_data(sheet_name=\"2019\")\n",
    "compile_aqi_data(sheet_name=\"2020\")\n",
    "compile_aqi_data(sheet_name=\"2021\")\n",
    "compile_aqi_data(sheet_name=\"2022\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AAQ Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_aaq_data(df: pd.DataFrame, pollutant:str) -> pd.DataFrame:\n",
    "    adilabad_variants = ['adilabad', 'mandamarri']\n",
    "    nizamabad_variants = ['nizamabad', 'subhasnagar']\n",
    "    khammam_variants = ['khammam', 'Jalasouda']\n",
    "    warangal_variants = ['warangal', 'kuda', 'mee-seva']\n",
    "    karimnagar_variants = ['karimnagar', 'godavarikhani', 'DIC building']\n",
    "\n",
    "    # getting pattern for warangal and taking avg of the variants\n",
    "    warangal_pattern = '|'.join(warangal_variants)\n",
    "    warangal_df = df.loc[df.location.str.contains(warangal_pattern, case=False)].drop(columns='location').mean()\n",
    "    warangal_df['location'] = 'Warangal'\n",
    "\n",
    "    # doing the same for the karimnagar variants\n",
    "    karimnagar_pattern = '|'.join(karimnagar_variants)\n",
    "    karimnagar_df = df.loc[df.location.str.contains(karimnagar_pattern, case=False)].drop(columns='location').mean()\n",
    "    karimnagar_df['location'] = 'Karimnagar'\n",
    "\n",
    "    # keeping other required cities apart from karimnagar and warangal\n",
    "    location_pattern = '|'.join(adilabad_variants + nizamabad_variants + khammam_variants)\n",
    "    df = df.loc[df.location.str.contains(location_pattern, case=False)]\n",
    "    df = df.append([karimnagar_df, warangal_df], ignore_index=True)   \n",
    "\n",
    "    # stacking the dataframe\n",
    "    df = df.set_index(\"location\").stack().reset_index()\n",
    "    df.columns = [\"district\", \"month\", pollutant]\n",
    "\n",
    "    # renaming rows to have standard district names\n",
    "    df.loc[df['district'].str.contains('|'.join(adilabad_variants), case=False), 'district'] = \"Adilabad\"\n",
    "    df.loc[df['district'].str.contains('|'.join(nizamabad_variants), case=False), 'district'] = \"Nizamabad\"\n",
    "    df.loc[df['district'].str.contains('|'.join(khammam_variants), case=False), 'district'] = \"Khammam\"\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_aaq_csv(file_path: str):\n",
    "    # files_list = glob.glob('./AAQ_Data/' + '/*.xls')\n",
    "    year = file_path[-8:-4]\n",
    "    col_names = [\"location\", \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
    "    sheet_names = [\"SO2\", \"NOx\", \"PM10\", \"PM2.5\", \"NH3\"]\n",
    "    \n",
    "\n",
    "    main_df = pd.read_excel(file_path, sheet_name=sheet_names[0], header=5, usecols='C:O', names=col_names)\n",
    "    main_df = main_df[main_df['location'].notna()]\n",
    "    main_df[main_df.columns.drop('location')] = main_df[main_df.columns.drop('location')].apply(pd.to_numeric, errors='coerce')\n",
    "    main_df = compile_aaq_data(main_df, sheet_names[0])\n",
    "\n",
    "    for sheet in sheet_names[1:]:\n",
    "        df = pd.read_excel(file_path, sheet_name=sheet, header=5, usecols='C:O', names=col_names)\n",
    "        df = df[df['location'].notna()]\n",
    "        df[df.columns.drop('location')] = df[df.columns.drop('location')].apply(pd.to_numeric, errors='coerce')\n",
    "        df = compile_aaq_data(df, sheet)\n",
    "        main_df = pd.merge(main_df, df, on=['district', 'month'], how='outer')\n",
    "\n",
    "    main_df.to_csv(f'./AAQ_Data/monthly_aaq_data_{year}.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list = glob.glob('./AAQ_Data/' + '/*.xls')\n",
    "for file in files_list:\n",
    "    make_aaq_csv(file)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_csv(path: str, data: str) -> None:\n",
    "    # files_list = glob.glob(path + \"/*.csv\")\n",
    "    files_list = glob.glob(path + \"/daily*.csv\")\n",
    "\n",
    "    main_df = pd.read_csv(files_list[0])\n",
    "\n",
    "    # comment below line for daily results\n",
    "    # main_df[\"year\"] = files_list[0][-8:-4]\n",
    "    for i in range(1, len(files_list)):\n",
    "        df = pd.read_csv(files_list[i])\n",
    "        # df[\"year\"] = files_list[i][-8:-4]\n",
    "\n",
    "        main_df = pd.concat([main_df, df])\n",
    "    \n",
    "    # main_df.to_csv(f\"./monthly_{data}_data.csv\", header=True, index=False)\n",
    "    main_df.to_csv(f\"./daily_{data}_data.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_csv(\"./AQI_Data/\", \"aqi\")\n",
    "combine_csv(\"./Industry_Consumption/\", \"industry_consumption\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_csv(\"./Weather_Data/\", \"weather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_csv(\"./AAQ_Data/\", \"aaq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_csv('./Weather_Data/', 'weather')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0467c16d4e73dba61dcc106221b4b7c95372fa61fc6cc648ee99bca30bbaf279"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
